{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of linear regression, we are trying to find the best-fit line that represents the relationship between the input feature(s) and the output. The goal is to find the coefficients $\\theta_0$ and $\\theta_1$ that minimize the difference between the predicted values and the actual target values.\n",
    "\n",
    "Let's break down the equation step by step:\n",
    "\n",
    "1. $\\boldsymbol{\\theta}$: This is a column vector containing the coefficients of the linear regression model. In the case of simple linear regression (with one input feature), $\\boldsymbol{\\theta}$ consists of two elements: $\\theta_0$ (the y-intercept) and $\\theta_1$ (the slope).\n",
    "\n",
    "2. $\\textbf{x}_b$: This is a column vector containing the input features of the dataset. In the context of simple linear regression, $\\textbf{x}_b$ has two elements: a constant term of 1 (to account for the y-intercept) and the input feature $x$.\n",
    "\n",
    "3. $\\boldsymbol{\\theta} \\cdot \\textbf{x}_b$: This is the dot product of the coefficient vector $\\boldsymbol{\\theta}$ and the input feature vector $\\textbf{x}_b$. The dot product is a mathematical operation that takes two vectors of the same length and returns a scalar. It is calculated by multiplying corresponding elements of the two vectors and then summing up the results.\n",
    "\n",
    "4. $\\theta_0 \\cdot 1 + \\theta_1 \\cdot x$: This is the expanded form of the dot product. Since $\\textbf{x}_b$ contains a constant term of 1, the dot product simplifies to $\\theta_0 \\cdot 1 + \\theta_1 \\cdot x$, which is the equation of a straight line (the linear regression model) with slope $\\theta_1$ and y-intercept $\\theta_0$.\n",
    "\n",
    "In summary, the equation $\\boldsymbol{\\theta} \\cdot \\textbf{x}_b = \\theta_0 \\cdot 1 + \\theta_1 \\cdot x = \\theta_0 + \\theta_1 \\cdot x$ represents the linear regression model's prediction for a given input feature $x$. The dot product of the coefficient vector and the input feature vector gives us the predicted output value, which is a linear combination of the input feature and the model's coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, vector multiplication and matrix multiplication are different operations.\n",
    "\n",
    "Vector multiplication refers to the mathematical operation performed between two vectors to produce a scalar value. There are two common types of vector multiplication:\n",
    "\n",
    "1. Dot Product: The dot product of two vectors is a scalar value obtained by multiplying corresponding elements of the vectors and then summing up the results. For two vectors $\\mathbf{a}$ and $\\mathbf{b}$ of the same length $n$, the dot product is denoted as $\\mathbf{a} \\cdot \\mathbf{b}$ or $\\langle \\mathbf{a}, \\mathbf{b} \\rangle$.\n",
    "\n",
    "2. Cross Product: The cross product of two 3-dimensional vectors results in a new vector that is perpendicular to both input vectors. The cross product is denoted as $\\mathbf{a} \\times \\mathbf{b}$.\n",
    "\n",
    "Matrix multiplication, on the other hand, is an operation performed between two matrices to produce another matrix. For two matrices $A$ and $B$ to be multiplied, the number of columns in matrix $A$ must be equal to the number of rows in matrix $B$. The resulting matrix, denoted as $C = AB$, has dimensions equal to the number of rows of $A$ and the number of columns of $B$.\n",
    "\n",
    "Matrix multiplication is defined as follows: For matrices $A$ of size $m \\times p$ and $B$ of size $p \\times n$, the product matrix $C$ is obtained by multiplying the corresponding elements of each row of $A$ with the corresponding elements of each column of $B$ and then summing up the results.\n",
    "\n",
    "To summarize, vector multiplication produces a scalar or a new vector, while matrix multiplication produces another matrix. Vector multiplication is a special case of matrix multiplication where one of the matrices is a row or column vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"vector product\" typically refers to the cross product of two vectors in three-dimensional space. The cross product of two vectors results in a new vector that is perpendicular to the plane containing the original two vectors. This operation is only defined for 3-dimensional vectors.\n",
    "\n",
    "Given two 3-dimensional vectors $\\mathbf{a} = \\begin{bmatrix}a_1 \\\\ a_2 \\\\ a_3\\end{bmatrix}$ and $\\mathbf{b} = \\begin{bmatrix}b_1 \\\\ b_2 \\\\ b_3\\end{bmatrix}$, the cross product $\\mathbf{c} = \\mathbf{a} \\times \\mathbf{b}$ is calculated as follows:\n",
    "\n",
    "1. The cross product vector $\\mathbf{c}$ has three components, denoted as $c_1$, $c_2$, and $c_3$.\n",
    "\n",
    "2. Calculate $c_1$ as the determinant of the following 2x2 matrix:\n",
    "\n",
    "   $c_1 = a_2 \\cdot b_3 - a_3 \\cdot b_2$\n",
    "\n",
    "3. Calculate $c_2$ as the determinant of the following 2x2 matrix:\n",
    "\n",
    "   $c_2 = a_3 \\cdot b_1 - a_1 \\cdot b_3$\n",
    "\n",
    "4. Calculate $c_3$ as the determinant of the following 2x2 matrix:\n",
    "\n",
    "   $c_3 = a_1 \\cdot b_2 - a_2 \\cdot b_1$\n",
    "\n",
    "5. The resulting vector $\\mathbf{c} = \\begin{bmatrix}c_1 \\\\ c_2 \\\\ c_3\\end{bmatrix}$ is the cross product of the two input vectors $\\mathbf{a}$ and $\\mathbf{b}$.\n",
    "\n",
    "Geometrically, the cross product vector $\\mathbf{c}$ is perpendicular to both $\\mathbf{a}$ and $\\mathbf{b}$ and its direction follows the right-hand rule: If you curl the fingers of your right hand from vector $\\mathbf{a}$ to vector $\\mathbf{b}$, then your thumb points in the direction of the cross product vector $\\mathbf{c}$.\n",
    "\n",
    "The cross product has several important applications in physics, engineering, and geometry. It is used to calculate the area of parallelograms and the volume of parallelepipeds. It is also used in calculating torque, angular momentum, and magnetic fields in physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Let's break down the steps that lead to the squared residuals being converted into the product of vector transpose.\n",
    "\n",
    "First, let's recall the expression for the residuals:\n",
    "\n",
    "$$\n",
    "\\text{Residuals} = y - \\text{y_{pred}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the vector of true target values (actual house prices in the example),\n",
    "- $\\text{y_{pred}}$ is the vector of predicted target values based on the linear model.\n",
    "\n",
    "Both $y$ and $\\text{y_{pred}}$ are column vectors, and they have the same length (number of samples in the dataset).\n",
    "\n",
    "Now, let's focus on calculating the Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "MSE(\\boldsymbol{\\theta}) = \\frac{1}{n} \\cdot \\left( \\text{Residuals}^T \\cdot \\text{Residuals} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\boldsymbol{\\theta}$ represents the parameter vector $ \\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\ \\end{array}\\right] $\n",
    "- $n$ is the number of samples in the dataset.\n",
    "\n",
    "We want to compute the square of the residuals and then average them to get the MSE. Let's see how this is done step by step:\n",
    "\n",
    "1. Square of Residuals: $\\text{Residuals}^T \\cdot \\text{Residuals}$\n",
    "   - When you multiply a column vector by its transpose, you get a square matrix.\n",
    "   - The square matrix will have dimensions $n \\times n$, where $n$ is the number of samples in the dataset.\n",
    "   - Each element of the square matrix will be the product of the corresponding elements in the Residuals vector.\n",
    "\n",
    "2. Sum of Squares: \n",
    "   $$ \\text{Residuals}^T \\cdot \\text{Residuals} = \\sum_{i=1}^{n}( y_i - {\\textbf{y}^{(i)}_{pred}} ^i )^2 $$\n",
    "   \n",
    "   - In this step, the square matrix obtained from the previous step is summed element-wise, resulting in a single scalar value.\n",
    "\n",
    "3. Average: $\\frac{1}{n} \\cdot \\left( \\text{Residuals}^T \\cdot \\text{Residuals} \\right) = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} \\left( y_i - {\\textbf{y}^{(i)}_{pred}} \\right)^2$\n",
    "   - Finally, we divide the sum of squares by the number of samples $n$ to obtain the Mean Squared Error (MSE).\n",
    "\n",
    "The transpose operation allows us to convert the square of the residuals (vector) into a dot product (matrix multiplication). This is a useful transformation that enables us to express the MSE calculation using vectorized operations, making it more computationally efficient when dealing with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(m,b) = \\frac{1}{7} \\cdot \\sum_{i=1}^7{(y_i-{\\color{#26a6ed}(\\textbf{x}^{(i)}_b \\cdot \\boldsymbol{\\theta})})}^2\n",
    "\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
